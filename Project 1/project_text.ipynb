{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "### Инструкция по выполнению проекта\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "### Описание данных\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 1. Импорт библиотек\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "import nltk\n",
    "\n",
    "from tqdm import notebook\n",
    "from pymystem3 import Mystem\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Импортировал все необходимые библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Загрузка файла\n",
    "\n",
    "df_tweets = pd.read_csv('/datasets/toxic_comments.csv')\n",
    "\n",
    "## 2.1. Установил ширину столбца, чтобы была возможность просмотреть твиты\n",
    "pd.set_option('display.max_colwidth',200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Загрузил файл\n",
    "2. Установил ширину столбцов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 2)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      "text     159571 non-null object\n",
      "toxic    159571 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>159571.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>0.101679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>0.302226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               toxic\n",
       "count  159571.000000\n",
       "mean        0.101679\n",
       "std         0.302226\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         0.000000\n",
       "75%         0.000000\n",
       "max         1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tid...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember what page that's on?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the tools well.  · talk \"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Your vandalism to the Matt Shirvington article has been reverted.  Please don't do it again, or you will be banned.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to you. Anyway, I'm not intending to write anything in the article(wow they would jump on me for vandalism), I'm merely requesting that it be more encycl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>alignment on this subject and which are contrary to those of DuLithgow</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>\"\\nFair use rationale for Image:Wonju.jpg\\n\\nThanks for uploading Image:Wonju.jpg. I notice the image page specifies that the image is being used under fair use but there is no explanation or rati...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>bbq \\n\\nbe a man and lets discuss it-maybe over the phone?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>Hey... what is it..\\n@ | talk .\\nWhat is it... an exclusive group of some WP TALIBANS...who are good at destroying, self-appointed purist who GANG UP any one who asks them questions abt their ANTI...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>Before you start throwing accusations and warnings at me, lets review the edit itself-making ad hominem attacks isn't going to strengthen your argument, it will merely make it look like you are ab...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>Oh, and the girl above started her arguments with me. She stuck her nose where it doesn't belong. I believe the argument was between me and Yvesnimmo. But like I said, the situation was settled an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                       text  \\\n",
       "0   Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remo...   \n",
       "1                                                                                          D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)   \n",
       "2   Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about...   \n",
       "3   \"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tid...   \n",
       "4                                                                                                                                       You, sir, are my hero. Any chance you remember what page that's on?   \n",
       "5                                                                                                                                       \"\\n\\nCongratulations from me as well, use the tools well.  · talk \"   \n",
       "6                                                                                                                                                              COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK   \n",
       "7                                                                                       Your vandalism to the Matt Shirvington article has been reverted.  Please don't do it again, or you will be banned.   \n",
       "8   Sorry if the word 'nonsense' was offensive to you. Anyway, I'm not intending to write anything in the article(wow they would jump on me for vandalism), I'm merely requesting that it be more encycl...   \n",
       "9                                                                                                                                    alignment on this subject and which are contrary to those of DuLithgow   \n",
       "10  \"\\nFair use rationale for Image:Wonju.jpg\\n\\nThanks for uploading Image:Wonju.jpg. I notice the image page specifies that the image is being used under fair use but there is no explanation or rati...   \n",
       "11                                                                                                                                               bbq \\n\\nbe a man and lets discuss it-maybe over the phone?   \n",
       "12  Hey... what is it..\\n@ | talk .\\nWhat is it... an exclusive group of some WP TALIBANS...who are good at destroying, self-appointed purist who GANG UP any one who asks them questions abt their ANTI...   \n",
       "13  Before you start throwing accusations and warnings at me, lets review the edit itself-making ad hominem attacks isn't going to strengthen your argument, it will merely make it look like you are ab...   \n",
       "14  Oh, and the girl above started her arguments with me. She stuck her nose where it doesn't belong. I believe the argument was between me and Yvesnimmo. But like I said, the situation was settled an...   \n",
       "\n",
       "    toxic  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  \n",
       "5       0  \n",
       "6       1  \n",
       "7       0  \n",
       "8       0  \n",
       "9       0  \n",
       "10      0  \n",
       "11      0  \n",
       "12      1  \n",
       "13      0  \n",
       "14      0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3. Визуальный осмотр файлов\n",
    "\n",
    "print (df_tweets.shape)\n",
    "print (df_tweets.info())\n",
    "display (df_tweets.describe())\n",
    "display (df_tweets.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Есть очень длинные записи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 100776 entries, 0 to 159570\n",
      "Data columns (total 3 columns):\n",
      "text      100776 non-null object\n",
      "toxic     100776 non-null int64\n",
      "length    100776 non-null int64\n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 4. Функция по избавлению от длинных твитов (для Berta)\n",
    "\n",
    "def length (text):\n",
    "        if len (text) < 300:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "df_tweets['length'] = df_tweets['text'].apply(length)\n",
    "df_tweets = df_tweets.query('length == 0')\n",
    "print (df_tweets.info())\n",
    "#df_tweets = df_tweets.sample(n = 600)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Создал функцию по избавлению от длинных твитов. Bert больше 512 символов не переваривает + для скорости работы (все виснет)\n",
    "2. Также создал возможность делать сэмплы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     Explanation Why the edits made under my username Hardcore Metallica Fan were reverted They weren t vandalism just closure on some GAs after I voted at New York Dolls FAC And please don t remove th...\n",
       "1                                                                                                                      D aww He match this background colour I m seemingly stuck with Thanks talk January UTC\n",
       "2     Hey man I m really not trying to edit war It s just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page He seems to care more about th...\n",
       "4                                                                                                                                             You sir are my hero Any chance you remember what page that s on\n",
       "5                                                                                                                                                       Congratulations from me a well use the tool well talk\n",
       "6                                                                                                                                                                COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK\n",
       "7                                                                                              Your vandalism to the Matt Shirvington article ha been reverted Please don t do it again or you will be banned\n",
       "9                                                                                                                                      alignment on this subject and which are contrary to those of DuLithgow\n",
       "11                                                                                                                                                        bbq be a man and let discus it maybe over the phone\n",
       "14    Oh and the girl above started her argument with me She stuck her nose where it doesn t belong I believe the argument wa between me and Yvesnimmo But like I said the situation wa settled and I apol...\n",
       "16                                                                                                                                                        Bye Don t look come or think of comming back Tosser\n",
       "17                                                                                                                                                            REDIRECT Talk Voydan Pop Georgiev Chernodrinski\n",
       "18                                                                                        The Mitsurugi point made no sense why not argue to include Hindi on Ryo Sakazaki s page to include more information\n",
       "20    Regarding your recent edits Once again please read WP FILMPLOT before editing any more film article Your edits are simply not good with entirely too many unnecessary detail and very bad writing Pl...\n",
       "21                                                                                                                                                          Good to know About me yeah I m studying now Deepu\n",
       "23                                                                                                                                  The Signpost September Read this Signpost in full Single page Unsubscribe\n",
       "25                                           Radial symmetry Several now extinct lineage included in the Echinodermata were bilateral such a Homostelea or even asymmetrical such a Cothurnocystis Stylophora\n",
       "26    There s no need to apologize A Wikipedia article is made for reconciling knowledge about a subject from different source and you ve done history study and not archaeology study I guess I could sca...\n",
       "28                                                                                            Ok But it will take a bit of work but I can t quite picture it Do you have an example I can base it on the Duck\n",
       "29                                                                                                                                                A barnstar for you The Real Life Barnstar let u be the star\n",
       "30                                                                                                                   How could I post before the block expires The funny thing is you think I m being uncivil\n",
       "31                                                                                                                                         Not sure about a heading of Fight for Freedom what will it contain\n",
       "32                                                                                                                                                Praise looked at this article about month ago much improved\n",
       "34    Well not before the process but before how we do thing with subpages His RfA is listed on NoSeptember s page and you can find it if you look September I think I have my difference with El C to be ...\n",
       "38                                                            Hi Explicit can you block O Fenian for edit warring on the Giant s Causeway wp He ha made several edits which can only be described a terrorism\n",
       "40    Sure but the lead must briefly summarize Armenia s history I simply added what I found necessary If anyone think this or that sentence is redundant for the lead they are welcome to remove make edi...\n",
       "41               TFD I think we just eced I think we responded to each other without seeing each others response I added something in response to yours but don t know if you saw mine T C WP CHICAGO WP FOUR\n",
       "43                                                                                                                                                                     FUCK YOUR FILTHY MOTHER IN THE ASS DRY\n",
       "46                                                                                                                You had a point and it s now ammended with appropriate encyclopedic notability significance\n",
       "47                                                                                                     In other word you re too lazy to actually point anything out Until you change that approach the tag go\n",
       "Name: lemmas, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 5. Лемматизация\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmas (text):\n",
    "    text = re.sub(r'[^a-zA-Z ]', ' ', text) \n",
    "    text = \" \".join(text.split())\n",
    "    \n",
    "    word_list = nltk.word_tokenize(text)\n",
    "\n",
    "    lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "    return lemmatized_output\n",
    "df_tweets['lemmas'] = df_tweets['text'].apply(lemmas)\n",
    "\n",
    "display (df_tweets['lemmas'].head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Explanation edits made under username Hardcore Metallica were reverted They weren vandalism just closure some after voted York Dolls please remove template from talk page since retired\n",
       "1                                                                                                                       match this background colour seemingly stuck with Thanks talk January\n",
       "2                        really trying edit just that this constantly removing relevant information talking through edits instead talk page seems care more about formatting than actual info\n",
       "Name: lemmas_1, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Тренировка - убираю слова меньше 3-х символов\n",
    "\n",
    "def lem (lemmas):\n",
    "    word_list = nltk.word_tokenize(lemmas)\n",
    "    lemmatized_output = \" \".join(w for w in word_list if len(w) > 3) \n",
    "    return lemmatized_output\n",
    "\n",
    "df_tweets['lemmas_1'] = df_tweets['lemmas'].apply(lem)\n",
    "\n",
    "\n",
    "display (df_tweets['lemmas_1'].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Лемматизирую наши твиты\n",
    "2. Для лемматизации использую WorldNetLemmatizer. Mysystem, который был в тренажере не подходит, так как оказывается работает только с русскими фразами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 6. Разделяю выборки, очищаю от стоп-слов, обучаю tf_idf\n",
    "\n",
    "features_train, features_test, target_train, target_test = train_test_split(df_tweets['lemmas_1'],df_tweets['toxic'], test_size = 0.2)\n",
    "\n",
    "corpus_train = features_train.values.astype('U')\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "count_tf_idf = TfidfVectorizer(stop_words=stopwords)\n",
    "tf_idf_train = count_tf_idf.fit_transform(corpus_train)\n",
    "\n",
    "corpus_test = features_test.values.astype('U')\n",
    "tf_idf_test = count_tf_idf.transform(corpus_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Здесь все прошло без заминки\n",
    "\n",
    "Комментарии на замечания:\n",
    "1. Код подкорректировал"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Создаю признаки на обучающей и тестовой выборках\n",
    "\n",
    "features_train = tf_idf_train\n",
    "features_test = tf_idf_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Все сделано"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7658062102506547\n"
     ]
    }
   ],
   "source": [
    "# 8. Логистическая регрессия \n",
    "\n",
    "model = LogisticRegression(random_state=12345, multi_class='auto', solver='liblinear', class_weight='balanced')\n",
    "model.fit(features_train, target_train)\n",
    "predictions = model.predict(features_test)\n",
    "print (f1_score(target_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Все здорово. F1 лучше таргета.\n",
    "2. Посмотрим можно ли получить еще лучше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5709391949757351\n"
     ]
    }
   ],
   "source": [
    "# 9. Решающее дерево. \n",
    "\n",
    "model = DecisionTreeClassifier(random_state=12345, max_depth=7) \n",
    "model.fit(features_train, target_train)\n",
    "predictions = model.predict(features_test)\n",
    "print (f1_score(target_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Результат сильно хуже. Как обычно на решающем дереве."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 1 1\n",
      "0.4328657314629259 1 100\n",
      "0.5425975939111221 1 199\n",
      "0.6275579809004093 1 298\n",
      "0.6373137647320436 1 397\n",
      "0.0 100 1\n",
      "0.40541411537222044 100 100\n",
      "0.69375 100 199\n",
      "0.7503628447024673 100 298\n",
      "0.7640820174404903 100 397\n",
      "0.0 199 1\n",
      "0.4115569823434992 199 100\n",
      "0.7022544700699662 199 199\n",
      "0.7493940862821135 199 298\n",
      "0.7645808736717827 199 397\n",
      "0.0 298 1\n",
      "0.40541411537222044 298 100\n",
      "0.6975051975051975 298 199\n",
      "0.7509113001215066 298 298\n",
      "0.7637739418302198 298 397\n",
      "0.0 397 1\n",
      "0.4018087855297158 397 100\n",
      "0.6992715920915713 397 199\n",
      "0.7497575169738118 397 298\n",
      "0.7638428774254614 397 397\n"
     ]
    }
   ],
   "source": [
    "# 10. Случайный лес. \n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for i in range (1,400,99):\n",
    "    for j in range (1,400,99):\n",
    "        model = RandomForestClassifier(random_state=12345, n_estimators=i, max_depth = j)\n",
    "        model.fit(features_train, target_train) \n",
    "        predictions = model.predict(features_test)\n",
    "        print (f1_score(target_test, predictions), i , j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Вот это разочарование. Модель отказывается обучаться.\n",
    "\n",
    "New:\n",
    "После изменения векторизации ничего не поменялось"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.243461\n"
     ]
    }
   ],
   "source": [
    "# 10. CatBoost\n",
    "\n",
    "model = CatBoostClassifier(iterations=200, depth = 16)\n",
    "model.fit(features_train, target_train, verbose=1) \n",
    "predictions = model.predict(features_test)\n",
    "\n",
    "print (f1_score(target_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Catboost таргет выполнил, но проиграл Логистической регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46399019307385847 1 1 30\n",
      "0.46399019307385847 1 1 150\n",
      "0.46399019307385847 1 1 270\n",
      "0.46399019307385847 1 121 30\n",
      "0.46399019307385847 1 121 150\n",
      "0.46399019307385847 1 121 270\n",
      "0.46399019307385847 1 241 30\n",
      "0.46399019307385847 1 241 150\n",
      "0.46399019307385847 1 241 270\n",
      "0.7453714835296945 16 1 30\n",
      "0.7482667941668659 16 1 150\n",
      "0.7499404052443386 16 1 270\n",
      "0.7453714835296945 16 121 30\n",
      "0.7482667941668659 16 121 150\n",
      "0.7499404052443386 16 121 270\n",
      "0.7453714835296945 16 241 30\n",
      "0.7482667941668659 16 241 150\n",
      "0.7499404052443386 16 241 270\n",
      "0.761028544468035 31 1 30\n",
      "0.7719054242002781 31 1 150\n",
      "0.7614189659169952 31 1 270\n",
      "0.761028544468035 31 121 30\n",
      "0.7719054242002781 31 121 150\n",
      "0.7614189659169952 31 121 270\n",
      "0.761028544468035 31 241 30\n",
      "0.7719054242002781 31 241 150\n",
      "0.7614189659169952 31 241 270\n",
      "0.761028544468035 46 1 30\n",
      "0.7801321485532012 46 1 150\n",
      "0.7749144811858609 46 1 270\n",
      "0.761028544468035 46 121 30\n",
      "0.7801321485532012 46 121 150\n",
      "0.7749144811858609 46 121 270\n",
      "0.761028544468035 46 241 30\n",
      "0.7801321485532012 46 241 150\n",
      "0.7749144811858609 46 241 270\n",
      "0.761028544468035 61 1 30\n",
      "0.7881259913890778 61 1 150\n",
      "0.7815831254252663 61 1 270\n",
      "0.761028544468035 61 121 30\n",
      "0.7881259913890778 61 121 150\n",
      "0.7815831254252663 61 121 270\n",
      "0.761028544468035 61 241 30\n",
      "0.7881259913890778 61 241 150\n",
      "0.7815831254252663 61 241 270\n",
      "0.761028544468035 76 1 30\n",
      "0.7896396396396398 76 1 150\n",
      "0.7834912043301758 76 1 270\n",
      "0.761028544468035 76 121 30\n",
      "0.7896396396396398 76 121 150\n",
      "0.7834912043301758 76 121 270\n",
      "0.761028544468035 76 241 30\n",
      "0.7896396396396398 76 241 150\n",
      "0.7834912043301758 76 241 270\n",
      "0.761028544468035 91 1 30\n",
      "0.7913023985653442 91 1 150\n",
      "0.7859712230215828 91 1 270\n",
      "0.761028544468035 91 121 30\n",
      "0.7913023985653442 91 121 150\n",
      "0.7859712230215828 91 121 270\n",
      "0.761028544468035 91 241 30\n",
      "0.7913023985653442 91 241 150\n",
      "0.7859712230215828 91 241 270\n"
     ]
    }
   ],
   "source": [
    "# 11. И, наконец, - LGBM\n",
    "\n",
    "for q in range (1,100,15):\n",
    "    for z in range (1,300,120):\n",
    "        for w in range (30,300,120):       \n",
    "            model = LGBMClassifier(loss_function='RMSE', n_iterations = z, max_depth = q, num_leaves = w, min_data_in_leaf = 3)\n",
    "            model.fit(features_train, target_train)\n",
    "            predictions = model.predict(features_test)\n",
    "            print (f1_score(target_test, predictions), q, z, w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чуть чуть до таргета не дотянули"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "789de502ed794b65b02abfc518e4cdfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=150.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 12. Bert\n",
    "## к сожалению, на маленьком количестве сэмплов - F1 неудовлетворительный, при прибавлении kernel - зависает\n",
    "\n",
    "df_tweets = df_tweets.sample(n = 15000)\n",
    "\n",
    "\n",
    "tokenizer = transformers.BertTokenizer(\n",
    "    vocab_file='/datasets/ds_bert/vocab.txt')\n",
    "\n",
    "tokenized = df_tweets['text'].apply(\n",
    "    lambda x: tokenizer.encode(x, add_special_tokens=True))\n",
    "\n",
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len - len(i)) for i in tokenized.values])\n",
    "\n",
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "\n",
    "config = transformers.BertConfig.from_json_file(\n",
    "    '/datasets/ds_bert/bert_config.json')\n",
    "model = transformers.BertModel.from_pretrained(\n",
    "    '/datasets/ds_bert/rubert_model.bin', config=config)\n",
    "\n",
    "\n",
    "batch_size = 100\n",
    "embeddings = []\n",
    "for i in notebook.tqdm(range(padded.shape[0] // batch_size)):\n",
    "        batch = torch.LongTensor(padded[batch_size*i:batch_size*(i+1)]) \n",
    "        attention_mask_batch = torch.LongTensor(attention_mask[batch_size*i:batch_size*(i+1)])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = model(batch, attention_mask = attention_mask_batch)\n",
    "        \n",
    "        embeddings.append(batch_embeddings[0][:,0,:].numpy())\n",
    "        \n",
    "features = np.concatenate(embeddings)\n",
    "target = df_tweets['toxic']\n",
    "features_train, features_test, target_train, target_test = train_test_split(features,target, test_size = 0.5)\n",
    "\n",
    "model = LogisticRegression(random_state=12345, multi_class='auto', solver='liblinear', class_weight='balanced')\n",
    "model.fit(features_train, target_train)\n",
    "predictions = model.predict(features_test)\n",
    "print (f1_score(target_test, predictions))\n",
    "\n",
    "\n",
    "cat_features = features\n",
    "model = CatBoostClassifier(iterations=100, depth = 7)\n",
    "model.fit(features_train, target_train, verbose=5) \n",
    "predictions = model.predict(features_test)\n",
    "print (f1_score(target_test, predictions))\n",
    "\n",
    "model = RandomForestClassifier(random_state=12345, n_estimators=199, max_depth = 397)\n",
    "model.fit(features_train, target_train) \n",
    "predictions = model.predict(features_test)\n",
    "print (f1_score(target_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалось обучить модель с F1, равным почти 0.79.\n",
    "Задание выполнено.\n",
    "Победила Логистическая Регрессия.\n",
    "В целом по проекту все понятно, за исключением неожиданно плохих результатов Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Чек-лист проверки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x]  Jupyter Notebook открыт\n",
    "- [x]  Весь код выполняется без ошибок\n",
    "- [x]  Ячейки с кодом расположены в порядке исполнения\n",
    "- [x]  Данные загружены и подготовлены\n",
    "- [x]  Модели обучены\n",
    "- [x]  Значение метрики *F1* не меньше 0.75\n",
    "- [x]  Выводы написаны"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
